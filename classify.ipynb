{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Classifies the test data and generates the submissions.\n",
    "\"\"\"\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "__author__ = 'Marin Iuga'\n",
    "__copyright__ = 'Copyright 2018, Marin Iuga / Intertechnica Business Solutions SRL'\n",
    "__credits__ = ['Marin Iuga']\n",
    "__license__ = 'MIT'\n",
    "__version__ = '1.0'\n",
    "__maintainer__ = 'Marin Iuga'\n",
    "__email__ = 'marin.iuga@intertechnica.com'\n",
    "__status__ = 'Production'\n",
    "\n",
    "#configuration data \n",
    "image_height = 128//1\n",
    "image_width = 2*118//1\n",
    "training_image_count = 988\n",
    "testing_image_count = 659\n",
    "classes_count = 11\n",
    "\n",
    "data_root_path = './data/'\n",
    "\n",
    "training_image_data_file_path = data_root_path + 'image_train.data'\n",
    "training_labels_data_file_path = data_root_path + 'image_train_labels.csv'\n",
    "testing_data_file_path = data_root_path + 'image_test.data'\n",
    "\n",
    "testing_submission_file_path = data_root_path + 'submission_format.csv'\n",
    "submission_results_file_path =  data_root_path + 'submission_results.csv'\n",
    "\n",
    "def read_image(p_image_data_file_path, p_position, p_image_width, p_image_height) :\n",
    "    \"\"\"\n",
    "    Reads an image from an image data from a image data repository @see prepare_data.py\n",
    "    @params:\n",
    "        p_image_data_file_path - Required : the image data file path (String)\n",
    "        p_position - Required : second image source (int)\n",
    "        p_image_width - Required: the image width (int)\n",
    "        p_image_height - Required: the image height (int)\n",
    "    \n",
    "    @returns:\n",
    "        The image data (array)    \n",
    "    \"\"\"  \n",
    "    with open(p_image_data_file_path, \"rb\") as image_file :\n",
    "        image_file.seek(p_position * p_image_height* p_image_width)\n",
    "        data = image_file.read(p_image_height * p_image_width)\n",
    "    \n",
    "        data_b = np.frombuffer(data, dtype=np.uint8)\n",
    "\n",
    "    return np.asarray(data_b)\n",
    "\n",
    "def process_images(p_images, p_image_width, p_image_height) :\n",
    "    \"\"\"\n",
    "    Processes a set of images so it can be classified by the neurals network model\n",
    "    @params:\n",
    "        p_images - Required : the images to process (String)\n",
    "        p_image_width - Required: the image width (int)\n",
    "        p_image_height - Required: the image height (int)\n",
    "    \"\"\"  \n",
    "    #reshape according to inputs accepted by a Conv2d layer\n",
    "    processed_images = p_images.reshape(p_images.shape[0], p_image_height, p_image_width, 1)\n",
    "\n",
    "    #data normalization to max value (0-255 grayscale values)\n",
    "    processed_images = (processed_images * 1.0) /255\n",
    " \n",
    "    return processed_images\n",
    "  \n",
    "def read_labels(p_labels_file_path) :\n",
    "    \"\"\"\n",
    "    Reads the extracted training labels @see prepare_data.py\n",
    "    @params:\n",
    "        p_labels_file_path - Required : the data file path (String)\n",
    "    @returns:\n",
    "        A dataframe containing the read labels with the column [id] for ordinal id and [label] for the label value    \n",
    "    \"\"\" \n",
    "    labels = pd.read_csv(p_labels_file_path, header= None)\n",
    "    labels.columns = [\"id\", \"label\"]\n",
    "  \n",
    "    return labels\n",
    "\n",
    "def process_labels(p_labels) :\n",
    "    \"\"\"\n",
    "    Processes the read labels\n",
    "    @params:\n",
    "        p_labels - Required: the read labels (array)\n",
    "    @returns:\n",
    "        The processed labels (binarization - one hot-encoded)    \n",
    "    \"\"\"\n",
    "    processed_labels = LabelBinarizer().fit_transform(p_labels)\n",
    "    \n",
    "    return processed_labels\n",
    "\n",
    "def generate_train_set(\n",
    "    p_image_training_data_file_path, \n",
    "    p_labels_file_path, \n",
    "    p_train_set_size, \n",
    "    p_image_width, \n",
    "    p_image_height\n",
    ") :\n",
    "    \"\"\"\n",
    "    Generates the training data set\n",
    "    @params:\n",
    "        p_image_training_data_file_path - Required: the training image data file path (String)\n",
    "        p_labels_file_path - Required: the labels file path (String)\n",
    "        p_train_set_size - Required: the size of the training set (int)\n",
    "        p_image_width - Required: the image width (int)\n",
    "        p_image_height - Required: the image height (int)\n",
    "    @returns:\n",
    "        (train_labels_processed, train_images_processed) tuple wiht the the processed train labels (array) \n",
    "        and the processed train images (array)\n",
    "    \"\"\"\n",
    "    labels = read_labels(p_labels_file_path)\n",
    "    \n",
    "    labels_batch = np.zeros(p_train_set_size)\n",
    "    labels_batch = labels[\"label\"][0:p_train_set_size].values\n",
    "\n",
    "    images_batch = []\n",
    "  \n",
    "    for i in range(0, p_train_set_size) :\n",
    "        image_data = read_image(p_image_training_data_file_path, i, p_image_width, p_image_height)\n",
    "        images_batch.append(image_data.reshape(p_image_height, p_image_width))\n",
    "  \n",
    "    train_labels_processed = process_labels(labels_batch)\n",
    "  \n",
    "    train_images_processed = process_images(np.array(images_batch), p_image_width, p_image_height)\n",
    "  \n",
    "    return train_labels_processed, train_images_processed\n",
    "\n",
    "def generate_test_set(\n",
    "    p_test_image_data_file_path, \n",
    "    p_test_set_size, \n",
    "    p_image_width, \n",
    "    p_image_height\n",
    ") :\n",
    "    \"\"\"\n",
    "    Generates the test data set\n",
    "    @params:\n",
    "        p_test_image_data_file_path - Required: the testing image data file path (String)\n",
    "        p_test_set_size - Required: the size of the testing set (int)\n",
    "        p_image_width - Required: the image width (int)\n",
    "        p_image_height - Required: the image height (int)\n",
    "    @returns:\n",
    "        test_images_processed the processed test images (array)\n",
    "    \"\"\"\n",
    "    images_batch = []\n",
    "\n",
    "    for i in range(0, p_test_set_size) :\n",
    "        image_data = read_image(p_test_image_data_file_path, i, p_image_width, p_image_height)\n",
    "        images_batch.append(image_data.reshape(p_image_height, p_image_width))\n",
    "\n",
    "    test_images_processed = process_images(np.array(images_batch), p_image_width, p_image_height)\n",
    "\n",
    "    return test_images_processed  \n",
    "  \n",
    "  \n",
    "def create_model(p_image_width, p_image_height, p_num_classes) :\n",
    "    \"\"\"\n",
    "    Creates the compiled model for image classification.\n",
    "    @params:\n",
    "        p_image_width - Required: the image width (int)\n",
    "        p_image_height - Required: the image height (int)\n",
    "        p_num_classes - Required: the number of classes\n",
    "    @returns:\n",
    "      The created and compiled model (Model)        \n",
    "    \"\"\"\n",
    "    input_shape = (p_image_height, p_image_width, 1)\n",
    "\n",
    "    #we will use a sequential model for training \n",
    "    model = Sequential()\n",
    "\t\n",
    "    #CONV 3x3x32 => RELU => NORMALIZATION => MAX POOL 3x3 block\n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "    #CONV 3x3x64 => RELU => NORMALIZATION => MAX POOL 2x2 block\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #CONV 3x3x128 => RELU => NORMALIZATION => MAX POOL 2x2 block\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #FLATTEN => DENSE 1024 => RELU => NORMALIZATION block\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #final DENSE => SOFTMAX block for multi-label classification\n",
    "    model.add(Dense(p_num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    #using categorical_crossentropy loss function with adam optimizer\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "    p_model, \n",
    "    p_training_image_data, \n",
    "    p_trainging_labels, \n",
    "    p_batch_size = 32, \n",
    "    p_epochs_to_train = 50, \n",
    "    p_verbose_level = 2\n",
    ") :\n",
    "    \"\"\"\n",
    "    Trains the model using the train image data and train labels.\n",
    "    \n",
    "    @parameters:\n",
    "      p_model - Required: the Keras model to be trained (Model)\n",
    "      p_training_image_data - Required: the image data used for training (array)\n",
    "      p_training_labels - Required: the training labels used fo training (array)\n",
    "      p_batch_size - Optional, default 32: the batch size used for training (int)\n",
    "      p_epochs_to_train - Optional, default 50: number of training epochs (int)\n",
    "      p_verbose_level - Optional, default 2: the Keras verbose level (int)\n",
    "    \n",
    "    @returns:\n",
    "      The trained model (Model)\n",
    "    \"\"\"    \n",
    "    p_model.fit(\n",
    "        x = p_training_image_data, \n",
    "        y = p_trainging_labels, \n",
    "        batch_size = p_batch_size, \n",
    "        epochs = p_epochs_to_train,\n",
    "        shuffle = True,\n",
    "        verbose = p_verbose_level    \n",
    "    )\n",
    "    \n",
    "    return p_model\n",
    "\n",
    "def predict_labels(p_model, p_test_image_data, p_batch_size = 32) :\n",
    "    \"\"\"\n",
    "    Predicts the labels associated with the test data.\n",
    "    \n",
    "    @parameters:\n",
    "      p_model - Required: the Keras model to be used (Model)\n",
    "      p_test_image_data - Required: the image data used for testing (array)\n",
    "      p_batch_size - Optional, default 32: the batch size used for training (int)\n",
    "    \n",
    "    @returns:\n",
    "      The predicted label (array)\n",
    "    \"\"\"      \n",
    "    labels = p_model.predict_classes(p_test_image_data, p_batch_size)\n",
    "  \n",
    "    return labels\n",
    "\n",
    "def write_results(\n",
    "    p_testing_submission_file_path, \n",
    "    p_submission_results_file_path, \n",
    "    p_results\n",
    ") :\n",
    "    \"\"\"\n",
    "    Writes the result to the output file.\n",
    "    \n",
    "    @parameters:\n",
    "      p_testing_submission_file_path - Required: the path to the submission format (String)\n",
    "      p_submission_results_file_path - Required: the path to the output file (String)\n",
    "      p_results - Required: the results to be written in the outut file (array)\n",
    "    \"\"\"     \n",
    "    submission_structure = pd.read_csv(p_testing_submission_file_path)\n",
    "    submission_structure['appliance'] = p_results\n",
    "    submission_structure.to_csv(p_submission_results_file_path, index=False)\n",
    "  \n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    #prepare training data\n",
    "    logging.info('Reading training data ...')\n",
    "    train_labels, train_images = generate_train_set(\n",
    "        training_image_data_file_path, \n",
    "        training_labels_data_file_path, \n",
    "        training_image_count, \n",
    "        image_width, \n",
    "        image_height\n",
    "    )\n",
    "    logging.info('Reading training data DONE')\n",
    "    \n",
    "    #create and train model\n",
    "    logging.info('Creating model ...')\n",
    "    model = create_model (image_width, image_height, classes_count)\n",
    "    logging.info('Creating model DONE')\n",
    "\n",
    "    logging.info('Training model ... ')\n",
    "    model = train_model(model, train_images, train_labels, p_epochs_to_train = 50)\n",
    "    logging.info('Training model DONE')\n",
    "    \n",
    "    #create test data\n",
    "    logging.info('Reading testing data ...')\n",
    "    test_images = generate_test_set(\n",
    "      testing_data_file_path, \n",
    "      testing_image_count, \n",
    "      image_width, \n",
    "      image_height\n",
    "    )\n",
    "    logging.info('Reading testing data DONE')\n",
    "    \n",
    "    #predict labels for test data\n",
    "    logging.info('Predicting test data classes ...')\n",
    "    result = predict_labels(model, test_images)\n",
    "    logging.info('Predicting test data classes DONE')\n",
    "    \n",
    "    #write results\n",
    "    logging.info('Writing results ...')\n",
    "    write_results(\n",
    "        testing_submission_file_path, \n",
    "        submission_results_file_path, \n",
    "        result\n",
    "    )\n",
    "    logging.info('Writing results DONE')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
