{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "INFO:root:Reading training data ...\n",
      "INFO:root:Reading training data DONE\n",
      "INFO:root:Creating model ...\n",
      "INFO:root:Creating model DONE\n",
      "INFO:root:Training model ... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 91s - loss: 1.8406 - accuracy: 0.6094\n",
      "Epoch 2/50\n",
      " - 91s - loss: 0.4486 - accuracy: 0.8611\n",
      "Epoch 3/50\n",
      " - 88s - loss: 0.2811 - accuracy: 0.9115\n",
      "Epoch 4/50\n",
      " - 94s - loss: 0.2346 - accuracy: 0.9306\n",
      "Epoch 5/50\n",
      " - 90s - loss: 0.1650 - accuracy: 0.9549\n",
      "Epoch 6/50\n",
      " - 93s - loss: 0.0705 - accuracy: 0.9844\n",
      "Epoch 7/50\n",
      " - 90s - loss: 0.0608 - accuracy: 0.9774\n",
      "Epoch 8/50\n",
      " - 89s - loss: 0.0307 - accuracy: 0.9948\n",
      "Epoch 9/50\n",
      " - 93s - loss: 0.0211 - accuracy: 0.9931\n",
      "Epoch 10/50\n",
      " - 94s - loss: 0.0136 - accuracy: 0.9983\n",
      "Epoch 11/50\n",
      " - 90s - loss: 0.0295 - accuracy: 0.9913\n",
      "Epoch 12/50\n",
      " - 90s - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 13/50\n",
      " - 105s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 14/50\n",
      " - 93s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 15/50\n",
      " - 92s - loss: 0.0105 - accuracy: 0.9983\n",
      "Epoch 16/50\n",
      " - 92s - loss: 0.0107 - accuracy: 0.9965\n",
      "Epoch 17/50\n",
      " - 87s - loss: 0.0194 - accuracy: 0.9913\n",
      "Epoch 18/50\n",
      " - 90s - loss: 0.0242 - accuracy: 0.9896\n",
      "Epoch 19/50\n",
      " - 95s - loss: 0.0288 - accuracy: 0.9896\n",
      "Epoch 20/50\n",
      " - 91s - loss: 0.0319 - accuracy: 0.9861\n",
      "Epoch 21/50\n",
      " - 96s - loss: 0.0554 - accuracy: 0.9826\n",
      "Epoch 22/50\n",
      " - 102s - loss: 0.0870 - accuracy: 0.9740\n",
      "Epoch 23/50\n",
      " - 112s - loss: 0.0794 - accuracy: 0.9705\n",
      "Epoch 24/50\n",
      " - 82s - loss: 0.0523 - accuracy: 0.9844\n",
      "Epoch 25/50\n",
      " - 80s - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      " - 79s - loss: 0.0073 - accuracy: 0.9983\n",
      "Epoch 27/50\n",
      " - 83s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      " - 84s - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      " - 82s - loss: 7.3869e-04 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      " - 84s - loss: 5.2664e-04 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      " - 82s - loss: 5.7563e-04 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      " - 79s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      " - 81s - loss: 5.8400e-04 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      " - 79s - loss: 5.7833e-04 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      " - 85s - loss: 7.4870e-04 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      " - 81s - loss: 3.0358e-04 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      " - 80s - loss: 3.0846e-04 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      " - 88s - loss: 2.2304e-04 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      " - 82s - loss: 2.1176e-04 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      " - 81s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      " - 80s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      " - 79s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      " - 82s - loss: 0.0105 - accuracy: 0.9948\n",
      "Epoch 44/50\n",
      " - 85s - loss: 0.0221 - accuracy: 0.9931\n",
      "Epoch 45/50\n",
      " - 80s - loss: 0.0316 - accuracy: 0.9896\n",
      "Epoch 46/50\n",
      " - 82s - loss: 0.0560 - accuracy: 0.9792\n",
      "Epoch 47/50\n",
      " - 82s - loss: 0.0908 - accuracy: 0.9722\n",
      "Epoch 48/50\n",
      " - 84s - loss: 0.0682 - accuracy: 0.9792\n",
      "Epoch 49/50\n",
      " - 82s - loss: 0.0376 - accuracy: 0.9913\n",
      "Epoch 50/50\n",
      " - 79s - loss: 0.0456 - accuracy: 0.9826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training model DONE\n",
      "INFO:root:Reading testing data ...\n",
      "INFO:root:Reading testing data DONE\n",
      "INFO:root:Predicting test data classes ...\n",
      "INFO:root:Predicting test data classes DONE\n",
      "INFO:root:Writing results ...\n",
      "INFO:root:Writing results DONE\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Classifies the test data and generates the submissions.\n",
    "\"\"\"\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "#configuration data \n",
    "image_height = 128//1\n",
    "image_width = 2*176//1\n",
    "training_image_count = 576\n",
    "testing_image_count = 384\n",
    "classes_count = 11\n",
    "\n",
    "data_root_path = \"./\"\n",
    "\n",
    "training_image_data_file_path = data_root_path + 'image_train.data'\n",
    "training_labels_data_file_path = data_root_path + 'image_train_labels.csv'\n",
    "testing_data_file_path = data_root_path + 'image_test.data'\n",
    "\n",
    "testing_submission_file_path = data_root_path + 'submission_format.csv'\n",
    "submission_results_file_path =  data_root_path + 'submission_results.csv'\n",
    "\n",
    "def read_image(p_image_data_file_path, p_position, p_image_width, p_image_height) :\n",
    "    \"\"\"\n",
    "    Reads an image from an image data from a image data repository @see prepare_data.py\n",
    "    @params:\n",
    "        p_image_data_file_path - Required : the image data file path (String)\n",
    "        p_position - Required : second image source (int)\n",
    "        p_image_width - Required: the image width (int)\n",
    "        p_image_height - Required: the image height (int)\n",
    "    \n",
    "    @returns:\n",
    "        The image data (array)    \n",
    "    \"\"\"  \n",
    "    with open(p_image_data_file_path, \"rb\") as image_file :\n",
    "        image_file.seek(p_position * p_image_height* p_image_width)\n",
    "        data = image_file.read(p_image_height * p_image_width)\n",
    "    \n",
    "        data_b = np.frombuffer(data, dtype=np.uint8)\n",
    "\n",
    "    return np.asarray(data_b)\n",
    "\n",
    "def process_images(p_images, p_image_width, p_image_height) :\n",
    "    \"\"\"\n",
    "    Processes a set of images so it can be classified by the neurals network model\n",
    "    @params:\n",
    "        p_images - Required : the images to process (String)\n",
    "        p_image_width - Required: the image width (int)\n",
    "        p_image_height - Required: the image height (int)\n",
    "    \"\"\"  \n",
    "    #reshape according to inputs accepted by a Conv2d layer\n",
    "    processed_images = p_images.reshape(p_images.shape[0], p_image_height, p_image_width, 1)\n",
    "\n",
    "    #data normalization to max value (0-255 grayscale values)\n",
    "    processed_images = (processed_images * 1.0) /255\n",
    " \n",
    "    return processed_images\n",
    "  \n",
    "def read_labels(p_labels_file_path) :\n",
    "    \"\"\"\n",
    "    Reads the extracted training labels @see prepare_data.py\n",
    "    @params:\n",
    "        p_labels_file_path - Required : the data file path (String)\n",
    "    @returns:\n",
    "        A dataframe containing the read labels with the column [id] for ordinal id and [label] for the label value    \n",
    "    \"\"\" \n",
    "    labels = pd.read_csv(p_labels_file_path, header= None)\n",
    "    labels.columns = [\"id\", \"label\"]\n",
    "  \n",
    "    return labels\n",
    "\n",
    "def process_labels(p_labels) :\n",
    "    \"\"\"\n",
    "    Processes the read labels\n",
    "    @params:\n",
    "        p_labels - Required: the read labels (array)\n",
    "    @returns:\n",
    "        The processed labels (binarization - one hot-encoded)    \n",
    "    \"\"\"\n",
    "    processed_labels = LabelBinarizer().fit_transform(p_labels)\n",
    "    \n",
    "    return processed_labels\n",
    "\n",
    "def generate_train_set(\n",
    "    p_image_training_data_file_path, \n",
    "    p_labels_file_path, \n",
    "    p_train_set_size, \n",
    "    p_image_width, \n",
    "    p_image_height\n",
    ") :\n",
    "    \"\"\"\n",
    "    Generates the training data set\n",
    "    @params:\n",
    "        p_image_training_data_file_path - Required: the training image data file path (String)\n",
    "        p_labels_file_path - Required: the labels file path (String)\n",
    "        p_train_set_size - Required: the size of the training set (int)\n",
    "        p_image_width - Required: the image width (int)\n",
    "        p_image_height - Required: the image height (int)\n",
    "    @returns:\n",
    "        (train_labels_processed, train_images_processed) tuple wiht the the processed train labels (array) \n",
    "        and the processed train images (array)\n",
    "    \"\"\"\n",
    "    labels = read_labels(p_labels_file_path)\n",
    "    \n",
    "    labels_batch = np.zeros(p_train_set_size)\n",
    "    labels_batch = labels[\"label\"][0:p_train_set_size].values\n",
    "\n",
    "    images_batch = []\n",
    "  \n",
    "    for i in range(0, p_train_set_size) :\n",
    "        image_data = read_image(p_image_training_data_file_path, i, p_image_width, p_image_height)\n",
    "        images_batch.append(image_data.reshape(p_image_height, p_image_width))\n",
    "  \n",
    "    train_labels_processed = process_labels(labels_batch)\n",
    "  \n",
    "    train_images_processed = process_images(np.array(images_batch), p_image_width, p_image_height)\n",
    "  \n",
    "    return train_labels_processed, train_images_processed\n",
    "\n",
    "def generate_test_set(\n",
    "    p_test_image_data_file_path, \n",
    "    p_test_set_size, \n",
    "    p_image_width, \n",
    "    p_image_height\n",
    ") :\n",
    "    \"\"\"\n",
    "    Generates the test data set\n",
    "    @params:\n",
    "        p_test_image_data_file_path - Required: the testing image data file path (String)\n",
    "        p_test_set_size - Required: the size of the testing set (int)\n",
    "        p_image_width - Required: the image width (int)\n",
    "        p_image_height - Required: the image height (int)\n",
    "    @returns:\n",
    "        test_images_processed the processed test images (array)\n",
    "    \"\"\"\n",
    "    images_batch = []\n",
    "\n",
    "    for i in range(0, p_test_set_size) :\n",
    "        image_data = read_image(p_test_image_data_file_path, i, p_image_width, p_image_height)\n",
    "        images_batch.append(image_data.reshape(p_image_height, p_image_width))\n",
    "\n",
    "    test_images_processed = process_images(np.array(images_batch), p_image_width, p_image_height)\n",
    "\n",
    "    return test_images_processed  \n",
    "  \n",
    "  \n",
    "def create_model(p_image_width, p_image_height, p_num_classes) :\n",
    "    \"\"\"\n",
    "    Creates the compiled model for image classification.\n",
    "    @params:\n",
    "        p_image_width - Required: the image width (int)\n",
    "        p_image_height - Required: the image height (int)\n",
    "        p_num_classes - Required: the number of classes\n",
    "    @returns:\n",
    "      The created and compiled model (Model)        \n",
    "    \"\"\"\n",
    "    input_shape = (p_image_height, p_image_width, 1)\n",
    "\n",
    "    #we will use a sequential model for training \n",
    "    model = Sequential()\n",
    "\t\n",
    "    #CONV 3x3x32 => RELU => NORMALIZATION => MAX POOL 3x3 block\n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "    #CONV 3x3x64 => RELU => NORMALIZATION => MAX POOL 2x2 block\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #CONV 3x3x128 => RELU => NORMALIZATION => MAX POOL 2x2 block\n",
    "    model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #FLATTEN => DENSE 1024 => RELU => NORMALIZATION block\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #final DENSE => SOFTMAX block for multi-label classification\n",
    "    model.add(Dense(p_num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    #using categorical_crossentropy loss function with adam optimizer\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "    p_model, \n",
    "    p_training_image_data, \n",
    "    p_trainging_labels, \n",
    "    p_batch_size = 32, \n",
    "    p_epochs_to_train = 50, \n",
    "    p_verbose_level = 2\n",
    ") :\n",
    "    \"\"\"\n",
    "    Trains the model using the train image data and train labels.\n",
    "    \n",
    "    @parameters:\n",
    "      p_model - Required: the Keras model to be trained (Model)\n",
    "      p_training_image_data - Required: the image data used for training (array)\n",
    "      p_training_labels - Required: the training labels used fo training (array)\n",
    "      p_batch_size - Optional, default 32: the batch size used for training (int)\n",
    "      p_epochs_to_train - Optional, default 50: number of training epochs (int)\n",
    "      p_verbose_level - Optional, default 2: the Keras verbose level (int)\n",
    "    \n",
    "    @returns:\n",
    "      The trained model (Model)\n",
    "    \"\"\"    \n",
    "    p_model.fit(\n",
    "        x = p_training_image_data, \n",
    "        y = p_trainging_labels, \n",
    "        batch_size = p_batch_size, \n",
    "        epochs = p_epochs_to_train,\n",
    "        shuffle = True,\n",
    "        verbose = p_verbose_level    \n",
    "    )\n",
    "    \n",
    "    return p_model\n",
    "\n",
    "def predict_labels(p_model, p_test_image_data, p_batch_size = 32) :\n",
    "    \"\"\"\n",
    "    Predicts the labels associated with the test data.\n",
    "    \n",
    "    @parameters:\n",
    "      p_model - Required: the Keras model to be used (Model)\n",
    "      p_test_image_data - Required: the image data used for testing (array)\n",
    "      p_batch_size - Optional, default 32: the batch size used for training (int)\n",
    "    \n",
    "    @returns:\n",
    "      The predicted label (array)\n",
    "    \"\"\"      \n",
    "    labels = p_model.predict_classes(p_test_image_data, p_batch_size)\n",
    "  \n",
    "    return labels\n",
    "\n",
    "def write_results(\n",
    "    p_testing_submission_file_path, \n",
    "    p_submission_results_file_path, \n",
    "    p_results\n",
    ") :\n",
    "    \"\"\"\n",
    "    Writes the result to the output file.\n",
    "    \n",
    "    @parameters:\n",
    "      p_testing_submission_file_path - Required: the path to the submission format (String)\n",
    "      p_submission_results_file_path - Required: the path to the output file (String)\n",
    "      p_results - Required: the results to be written in the outut file (array)\n",
    "    \"\"\"     \n",
    "    submission_structure = pd.read_csv(p_testing_submission_file_path)\n",
    "    submission_structure['appliance'] = p_results\n",
    "    submission_structure.to_csv(p_submission_results_file_path, index=False)\n",
    "  \n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    #prepare training data\n",
    "    logging.info('Reading training data ...')\n",
    "    train_labels, train_images = generate_train_set(\n",
    "        training_image_data_file_path, \n",
    "        training_labels_data_file_path, \n",
    "        training_image_count, \n",
    "        image_width, \n",
    "        image_height\n",
    "    )\n",
    "    logging.info('Reading training data DONE')\n",
    "    \n",
    "    #create and train model\n",
    "    logging.info('Creating model ...')\n",
    "    model = create_model (image_width, image_height, classes_count)\n",
    "    logging.info('Creating model DONE')\n",
    "\n",
    "    logging.info('Training model ... ')\n",
    "    model = train_model(model, train_images, train_labels, p_epochs_to_train = 50)\n",
    "    logging.info('Training model DONE')\n",
    "    \n",
    "    #create test data\n",
    "    logging.info('Reading testing data ...')\n",
    "    test_images = generate_test_set(\n",
    "      testing_data_file_path, \n",
    "      testing_image_count, \n",
    "      image_width, \n",
    "      image_height\n",
    "    )\n",
    "    logging.info('Reading testing data DONE')\n",
    "    \n",
    "    #predict labels for test data\n",
    "    logging.info('Predicting test data classes ...')\n",
    "    result = predict_labels(model, test_images)\n",
    "    logging.info('Predicting test data classes DONE')\n",
    "    \n",
    "    #write results\n",
    "    logging.info('Writing results ...')\n",
    "    write_results(\n",
    "        testing_submission_file_path, \n",
    "        submission_results_file_path, \n",
    "        result\n",
    "    )\n",
    "    logging.info('Writing results DONE')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
